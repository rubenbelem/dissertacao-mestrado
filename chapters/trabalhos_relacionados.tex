\chapter{Trabalhos relacionados}\label{sec:referencias}
\label{sec:related_work}

O problema de CATE foi primeiramente estudado por \cite{chaudhuri2009extending} e \cite{ji2009efficient}. Ambos propuseram soluções baseadas em computar, de forma incremental, um conjunto de nós ativos de uma \textit{Trie}, a qual funciona como índice para a base de consultas que poderão ser sugeridas. A ideia geral comum entre os dois algoritmos é a de que os algoritmos calculam um conjunto de ``nós ativos'' que correspondem a todos os prefixos encontrados na \textit{Trie} similares a cada letra nova de um prefixo de consulta $p$ digitado pelo usuário, dentro de um limite de distância de edição $\tau$. A partir desses prefixos calculados, é possível recuperar na própria árvore o restante do texto das consultas que serão sugeridas.

Chaudhuri et al. \cite{chaudhuri2009extending} apresentam um método de complementação automática de consultas que leva em consideração erros de digitação no prefixo. Antes disso, para cenários em que havia uma tabela de frases para pesquisar, a forma mais comum de preenchimento automático era a comparação exata de caracteres. Dois algoritmos de complementação automática tolerantes à falha de digitação foram propostos \citep{chaudhuri2009extending}. O primeiro é baseado nos algoritmos de distância de edição em q-gramas\citep{arasu2006efficient, chaudhuri2006primitive, xiao2008ed}. O segundo é um algoritmo baseado em \textit{Trie}, que mantém um conjunto de \textit{nós ativos} de forma incremental para processar as complementações. O método utiliza distância de edição clássica como medida de similaridade entre cadeias de caracteres. Esse algoritmo é muito similar ao proposto por \cite{ji2009efficient}, porém, realiza uma etapa a mais, que consiste em pré-computar o conjunto de nós ativos para todas as possíveis consultas com até certo tamanho. Uma vez que o número de nós ativos pode ser muito grande ao realizar complementação de consultas tolerante a erros, essa estratégia ajuda a reduzir o tempo de processamento das consultas.

Para abordar o problema de CATE Ji et. al \citep{ji2009efficient} propõem um algoritmo chamado \textit{ICAN} que computa nós ativos de forma incremental, ou seja, processa o prefixo consultado caractere a caractere, como mencionado anteriormente. Nosso método proposto na seção~\ref{sec:metodo} utiliza uma otimização desse algortimo, o ICPAN \citep{li2011efficient}. Portanto, ambos serão descritos com mais detalhes a seguir, e também serão mais aprofundados na seção~\ref{sec:ref_teorico}. 

Seja $p$ um prefixo consultado e $\tau$ um limite de distância de edição. Um nó $n$ da \textit{Trie} é chamado \textit{nó ativo de p em relação à $\tau$} quando a distância de edição entre a cadeia de caracteres de $n$ e a de $p$ estiver dentro do limite $\tau$, ou em outras palavras, $ed(n, p) \leq \tau$. Os nós-folha da subárvore com raiz em $n$ são denominados ``palavras similares à $p$'' \citep{ji2009efficient}. 

Considerando uma consulta de prefixo $p$ com apenas uma palavra-chave, para processar os nós ativos de forma eficiente é necessário computar e armazenar um conjunto de tuplas $\Phi_{p} = \{ \langle n, \xi_{n} \rangle \}$ tal que (1) cada $n$ é um nó ativo em relação à $p$ com $\xi_{n} = ed(n, p) \leq \tau)$ e (2) quaisquer nós ativos de $p$ estão presentes em $\Phi_{p}$, que é o \textit{conjunto de nós ativos de p} \citep{ji2009efficient}. Quando o usuário digita mais um caractere $p + 1$ após ter digitado $p$, o conjunto $\Phi_{p}$ de nós ativos de $p$ pode ser usado para computar o conjunto $\Phi_{p + 1}$ de nós ativos da nova consulta.

Uma desvantagem do algoritmo ICAN é que pode ser bastante custoso manter o conjunto de nós ativos para grandes quantidades de dados. Para atenuar esse problema, Li et al. \citep{li2011efficient} desenvolveram um método otimizado denominado \textit{ICPAN}, que realiza a poda de nós ativos desnecessários durante o processamento de consultas enquanto continua conseguindo computar todos os itens similares ao prefixo consultado. Isso permite a redução do espaço necessário para armazenar os nós ativos computados e também melhora o desempenho da busca, uma vez que não é necessário verificar todos os nós ativos para a computação incremental.

O ICPAN define um subconjunto dos nós ativos que pode ser usado para computar todas as palavras similares à consulta de forma eficiente e incremental. Para isso, Li et al. \citep{li2011efficient} estabelecem a seguinte observação: dado um nó ativo $n$ de $p$, se para qualquer transformação de $n$ para $p$ com $ed(n,p)$ operações de edição, não houver uma operação de correspondência (caracteres iguais) no último caractere de $n$, e for possível deletar o último caractere de $n$, o nó $n$ não é mantido, e sim o seu nó pai. Assim, surge o conceito de \textit{nó pivô ativo}.

Considerando um prefixo de consulta $p$, um nó da \textit{Trie} $n$ é um nó pivô ativo de $p$ em relação à $\tau$ se, e somente se (1) $n$ é um nó ativo de $p$ e (2) se existe uma transformação de $p$ para $n$ com $ed(n,p)$ operações, e a operação no último caractere de $n$ é uma correspondência. Em outras palavras, a operação nesse caractere de $n$ não é nem uma deleção $ed(n,p) \neq ed(n',p) + 1$ e nem uma substituição $ed(n,p) \neq ed(n', p') + 1$, onde $n'$ e $p'$ são respectivamente os prefixos de $n$ e $p$ que não contêm o último caractere.

Dada uma consulta de prefixo $p_{x}$, de forma análoga ao ICAN, é preciso computar e armazenar um conjunto de tuplas $\Psi_{p_{x}} = \{ \langle n, \xi_{n}^{p_{x}}, p_{i}, \xi_{n}^{p_{i}} \rangle \}$ \citep{li2011efficient}. Em cada tupla, $n$ é um nó pivô ativo de $p_{x}$. O elemento $p_{i}$ é um prefixo de $p_{x}$ tal que os últimos caracteres de $n$ e $p_{i}$ são iguais. Se não existir tal prefixo, então $p_{i} = \epsilon$ (cadeia de caracteres vazia). Se existirem múltiplos prefixos com essa condição, somente aquele com o menor comprimento é selecionado. $\xi_{n}^{p_{i}} \leq \tau$ é uma distância de transformação entre o nó $n$ para $p_{i}$ com uma operação de correspondência entre seus últimos caracteres. $\xi_{n}^{p_{x}} \leq \tau$ é a distância de transformação entre o nó $n$ para $p_{x}$ obtida ao primeiro transformar o nó $n$ para $p_{i}$ e então inserir os caracteres após $p_{i}$. O ICPAN é um algoritmo que computa $\Psi{p_{x}}$ e garante que cada nó pivô ativo de $p_{x}$ aparece como o $n$ em uma tupla de $\Psi{p_{x}}$.

Em contrapartida, Xiao et al. \citep{xiao2013efficient} argumentam que a eficiência dos métodos supramencionados criticamente depende da quantidade de nós ativos, e que tal número é tipicamente muito grande e também linear no tamanho da base de sugestões indexada, ou até mesmo exponencial no tamanho do alfabeto no pior caso. Então, os autores resolvem seguir em outra direção, investigando se é possível melhorar drasticamente o desempenho do processamento da consulta ao processar previamente a base e construir um índice de grandes proporções. Para isso, propuseram uma solução denominada IncNGTrie que consiste em indexar as ``variações marcadas de deleção'' (VMD) das sugestões em uma Trie e então manter um pequeno conjunto de nós ativos durante o processamento. 

O conteúdo indexado não é o texto original das sugestões de consulta e sim suas variantes marcadas de $\tau$-deleções, as quais são geradas ao deletar no máximo $\tau$ caracteres dos textos. Então, quando o usuário digita uma consulta o método computa as variantes da mesma e as pesquisa na árvore Trie. Essa técnica pode ser realizada de forma incremental e eficiente, mantendo um pequeno conjunto de nós ativos (reduzindo o tamanho em até 3 ordens de grandeza comparado aos métodos ICAN e ICPAN, por exemplo). 
Apesar dessa abordagem ter reduzido o tempo necessário para processar as consultas em comparação às outras soluções presentes na literatura, o tamanho dos índices também se demonstrou muito maior que dos outros métodos, representando uma severa restrição à utilização do IncNGTrie quanto à memória. Qin et al. \citep{qin2020efficient} apresentam o IncNGTrie+, uma melhoria do IncNGTrie que reduziu tanto o tempo de processamento quanto o tamanho do índice construindo, diminuindo a quantidade de memória requerida, apesar de ainda precisarem de mais memória do que os métodos BEVA e META, por exemplo.

Zhou et al. \citep{zhou2016beva} propõem uma estrutura geral que engloba métodos da complementação automática existentes, o ``BEVA'', caracterizando diferentes classes de algoritmos e a quantidade mínima de informações que eles precisam manter para diversas restrições. É proposta também uma nova estratégia de armazenamento de ``nós ativos de fronteira'' (menor conjunto de nós ativos possível, sem redundâncias), eliminando inteiramente os relacionamentos entre nós pais e filhos. Essa abordagem realiza computação de distância de edição através de uma nova estrutura de dados chamada ``\textit{edit vector automaton}'' (EVA), conseguindo calcular novos nós ativos e seus estados associados de forma eficiente através de pesquisas de tabela. O modelo é capaz de suportar limiares de grande distância através de um autômato. Os estudos do trabalho indicam que o método supera as abordagens até então existentes, tanto em tempo quanto na eficiência do espaço.

Deng et al. \citep{deng2016meta} desenvolveram um método denominado ``META'', uma estrutura baseada em correspondências que calcula as respostas com base em caracteres correspondentes entre os dados das consultas e os dados indexados. A estrutura desenvolvida é capaz de reduzir cálculos redundantes, organizando-os através de índices de árvore compacta. Para processar as respostas às consultas foi proposto um método incremental que responde eficientemente consultas \textit{top-k}. 

Todos os trabalhos supracitados utilizam \textit{Trie} como seu principal índice para realizar a computação de prefixos similares ao prefixo de consulta digitado pelo usuário. Isso fornece uma forte base para considerar que essa estrutura de dados realmente é vantajosa para o problema de CATE em se tratando do tempo de processamento. No entanto, as árvores \textit{Trie} tendem a consumir uma alta quantidade de memória quando a base de consultas indexada é muito grande e não há muitos prefixos em comum entre os itens. Uma das formas de contornar esse problema é utilizar uma estratégia de indexação e busca em dois níveis.

Manber et al. \citep{manber1994glimpse} apresentam o ``GLIMPSE'' (\textit{GLobal IMPlicit Search}) ou ``Busca Implícita Global'', uma ferramenta que possibilita a realização de consultas para sistemas de arquivos com baixa indexação, onde apenas 2\% e 4\% do tamanho do texto é indexado. O GLIMPSE apresentou uma abordagem até então nova para a indexação e realização de consultas de arquivos, denominada de ``busca em dois níveis''.

A ideia da busca em dois níveis aplicada no GLIMPSE consiste em combinar índices invertidos completos com busca sequencial sem indexação. Essa abordagem se baseia na observação de que a busca sequencial é rápida o suficiente para textos de vários \textit{megabytes} de tamanho, e que por isso não é necessário indexar todas as palavras do documento com suas localizações exatas. Na busca em dois níveis, o índice não fornece os locais exatos das palavras, mas apenas uma indicação da região onde a resposta possa ser encontrada, funcionando como uma espécia de ``filtro''. Nessa abordagem no entanto, apesar de economizar bastante espaço de indexação, a velocidade de busca se mostrou inferior aos modelos que utilizam apenas indexadores baseados em lista invertida.

Navarro et al. ~\citep{navarro2000adding} estudaram uma possível combinação entre índice invertido completo e pesquisa sequencial sem indexação. A pesquisa completa em coleções de texto é feita com o uso de um índice que aponta para blocos em vez de apontar para cada posição no texto. Primeiramente, a pesquisa é realizada no índice para detectar blocos que podem corresponder à consulta e, em seguida, no segundo nível é feita uma pesquisa sequencial para encontrar a lista real de
ocorrências no texto.

Quanto ao índice utilizado no primeiro nível, é possível utilizar índices baseados em árvores \textit{Trie}. Heinz et al. \citep{heinz2002burst} propõem a estrutura \textit{Burst Trie}, a qual pode ser considerada como um outro exemplo de índice de dois níveis, porém é aplicada em outro contexto. As \textit{Burst \textit{Trie}s} são coleções de pequenas estruturas de dados denominadas ``recipientes''. Esses ``recipientes'' são análogos ao segundo nível nos moldes explicados anteriormente, e são acessados através de uma \textit{Trie} comum, a qual pode ser considerada como um primeiro nível.

Um problema comum a todos os métodos de complementação automática de consultas tolerante a erros supracitados é o alto consumo de memória, uma vez que todos utilizam a estrutura \textit{Trie} para indexar todo o texto de cada consulta da base de dados. Tendo como influência a combinação de índice invertido e pesquisa sequencial sem indexação (``busca em dois níveis''), uma forma de reduzir a quantidade de memória necessária seria indexar somente alguns caracteres iniciais de cada consulta na árvore \textit{Trie}, e utilizar de pesquisa sequencial caso o processamento precise ir além das folhas da árvore. 

Costa Xavier \citep{xavier2019} realizou um trabalho preliminar que confirmou ser possível utilizar uma abordagem de busca em dois níveis para o problema de CATE. Nesse método, define-se uma quantidade fixa $L$ de caracteres (profundidade máxima da \textit{Trie}), e então somente os $L$ primeiros caracteres de cada consulta são indexados na árvore. Dado um prefixo de consulta $P$ tal que $|P| > L$, a computação de sugestões é dividida em duas etapas, ou níveis. O primeiro nível utiliza o algoritmo ICAN \citep{ji2009efficient} para processar os $L$ primeiros caracteres de $P$. Após o término, resta um conjunto de nós ativos da \textit{Trie} referentes à sub-cadeia  $P[1..L]$. Então, no segundo nível, é realizada uma busca sequencial em cada consulta referenciada por cada nó ativo. 

Após esse primeiro trabalho, Gama Ferreira \citep{berg2020} aprimorou significativamente a busca em dois níveis ao utilizar utilizar o mesmo algoritmo (BEVA) de busca nos dois níveis da \textit{Trie}. Além disso, o trabalho propõe uma estratégia de inserção de chaves em seu índice \textit{Trie} que acelera o processamento das consultas por utilizar o sistema de cache das máquinas de maneira mais eficiente. Essa mudança na forma de inserir chaves tem potencial para causar melhorias também em outros algoritmos baseados em \textit{Trie} presentes na literatura. A busca sequencial no segundo nível deste método é mais eficiente que a de \cite{xavier2019}, porque os estados de computação de distância de edição já existentes no primeiro nível podem ser usados para poupar redundâncias computacionais no segundo nível. Tal método atingiu uma ótima economia de memória em comparação ao algoritmo original BEVA, com o aditivo de conseguir manter um desempenho semelhante. 

Os trabalhos de \cite{xavier2019} e \cite{berg2020} demonstraram que é possível combinar a estratégia de busca em índices \textit{Trie} com busca sequencial para resolver o problema de CATE. Tal combinação reduz significativamente o uso de memória, enquanto mantém o tempo de processamento dentro do limite ideal de $100ms$ para uma boa parte dos casos experimentados. Nesta dissertação, exploramos a possibilidade de realizar otimizações em alguns casos específicos de processamento de consultas no segundo nível ao combinar busca sequencial com busca binária, visando obter ganhos de desempenho.
